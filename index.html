



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="K-Mean Clustering dan K-Nearest Neighbor">
      
      
        <link rel="canonical" href="https://github.com/aguskurr/">
      
      
        <meta name="author" content="Agus Kurniawan">
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.2.0">
    
    
      
        <title>Data Mining</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/application.750b69bd.css">
      
        <link rel="stylesheet" href="assets/stylesheets/application-palette.224b79ff.css">
      
      
        
        
        <meta name="theme-color" content="#3f51b5">
      
    
    
      <script src="assets/javascripts/modernizr.74668098.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="assets/fonts/material-icons.css">
    
    
    
      
        
<script>
  window.ga = window.ga || function() {
    (ga.q = ga.q || []).push(arguments)
  }
  ga.l = +new Date
  /* Setup integration and send page view */
  ga("create", "None", "auto")
  ga("set", "anonymizeIp", true)
  ga("send", "pageview")
  /* Register handler to log search on blur */
  document.addEventListener("DOMContentLoaded", () => {
    if (document.forms.search) {
      var query = document.forms.search.query
      query.addEventListener("blur", function() {
        if (this.value) {
          var path = document.location.pathname;
          ga("send", "pageview", path + "?q=" + this.value)
        }
      })
    }
  })
</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#k-means-clustering" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://github.com/aguskurr" title="Data Mining" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              Data Mining
            </span>
            <span class="md-header-nav__topic">
              Apa itu K-Means clustering ?
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/aguskurr/170441100075_KmeandanKNN" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    aguskurr/Data Mining
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
        

<nav class="md-tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  <li class="md-tabs__item">
    
      <a href="." title="Apa itu K-Means clustering ?" class="md-tabs__link md-tabs__link--active">
        Apa itu K-Means clustering ?
      </a>
    
  </li>

      
        
      
        
      
        
      
    </ul>
  </div>
</nav>
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://github.com/aguskurr" title="Data Mining" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    Data Mining
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/aguskurr/170441100075_KmeandanKNN" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    aguskurr/Data Mining
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
    <a href="." title="Apa itu K-Means clustering ?" class="md-nav__link md-nav__link--active">
      Apa itu K-Means clustering ?
    </a>
    
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="getting-started/" title="Implemantasi K-Mean clustering" class="md-nav__link">
      Implemantasi K-Mean clustering
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="customization/" title="Apa itu K-Nearest Neighbor ?" class="md-nav__link">
      Apa itu K-Nearest Neighbor ?
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="release-notes/" title="Implemantasi K-Nearest Neighbor with Python" class="md-nav__link">
      Implemantasi K-Nearest Neighbor with Python
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="k-means-clustering"><small>K-Means Clustering</small></h1>
<p>K-Means adalah suatu metode penganalisaan data atau metode Data Mining yang melakukan proses pemodelan tanpa supervisi (unsupervised) dan merupakan salah satu metode yang melakukan pengelompokan data dengan sistem partisi. Metode k-means berusaha mengelompokkan data yang ada ke dalam beberapa kelompok, dimana data dalam satu kelompok mempunyai karakteristik yang sama satu sama lainnya dan mempunyai karakteristik yang berbeda dengan data yang ada di dalam kelompok yang lain. Dengan kata lain, metode ini berusaha untuk meminimalkan variasi antar data yang ada di dalam suatu cluster dan memaksimalkan variasi dengan data yang ada di cluster lainnya.</p>
<p><strong>Objective function</strong> yang berusaha diminimalkan oleh k-means adalah:</p>
<p>J (U, V) = SUM (k=1 to N) SUM (i=1 to c) (a_ik * (x_k, v_i)^2)</p>
<p>dimana:
U : Matriks keanggotaan data ke masing-masing cluster yang berisikan nilai 0 dan 1
V : Matriks centroid/rata-rata masing-masing cluster
N : Jumlah data
c : Jumlah cluster
a_ik : Keanggotaan data ke-k ke cluster ke-i
x_k : data ke-k
v_i : Nilai centroid cluster ke-i</p>
<p><strong>Prosedur</strong> yang digunakan dalam melakukan optimasi menggunakan k-means adalah sebagai berikut:
Step 1. Tentukan jumlah cluster
Step 2. Alokasikan data ke dalam cluster secara random
Step 3. Hitung centroid/rata-rata dari data yang ada di masing-masing cluster.
Step 4. Alokasikan masing-masing data ke centroid/rata-rata terdekat
Step 5. Kembali ke Step 3, apabila masih ada data yang berpindah cluster atau apabila perubahan nilai centroid, ada yang di atas nilai threshold yang ditentukan atau apabila perubahan nilai pada objective function yang digunakan, di atas nilai threshold yang ditentukan</p>
<p>Centroid/rata-rata dari data yang ada di masing-masing cluster yang dihitung pada Step 3. didapatkan menggunakan rumus sebagai berikut:</p>
<p>v_ij = SUM (k=0 to N_i) (x_kj) / N_i</p>
<p>dimana:
i,k : indeks dari cluster
j : indeks dari variabel
v_ij : centroid/rata-rata cluster ke-i untuk variabel ke-j
x_kj : nilai data ke-k yang ada di dalam cluster tersebut untuk variabel ke-j
N_i : Jumlah data yang menjadi anggota cluster ke-i</p>
<p>Sedangkan pengalokasian data ke masing-masing cluster yang dilakukan pada Step 4. dilakukan secara penuh, dimana nilai yang memungkinkan untuk a_ik adalah 0 atau 1. Nilai 1 untuk data yang dialokasikan ke cluster dan nilai 0 untuk data yang dialokasikan ke cluster yang lain. Dalam menentukan apakah suatu data teralokasikan ke suatu cluster atau tidak, dapat dilakukan dengan menghitung jarak data tersebut ke masing-masing centroid/rata-rata masing-masing cluster. Dalam hal ini, a_ik akan bernilai 1 untuk cluster yang centroidnya terdekat dengan data tersebut, dan bernilai 0 untuk yang lainnya.</p>
<p>Referensi:
J. B. MacQueen (1967): “Some Methods for classification and Analysis of Multivariate Observations, Proceedings of 5-th Berkeley Symposium on Mathematical Statistics and Probability”, Berkeley, University of California Press, 1:281-297
J. A. Hartigan (1975) “Clustering Algorithms”. Wiley.
J. A. Hartigan and M. A. Wong (1979). “A K-Means Clustering Algorithm”. Applied Statistics 28 (1): 100–108.</p>
<p><strong>Cluster Validity Criterion</strong></p>
<p>Untuk menentukan jumlah cluster yang paling tepat, saat menggunakan metode k-means dapat dilakukan dengan beberapa cara. Salah satunya adalah dengan cara manual yang saya jelaskan dalam posting saya tentang <a href="https://yudiagusta.wordpress.com/2008/03/11/akurasi-hasil-pemodelan-k-means/">Akurasi Hasil Pemodelan K-Means</a> yang sering juga direfer sebagai <em>Bootstrapped Method</em>. Selain itu ada beberapa cara yang lain yang juga bisa digunakan seperti di bawah ini.</p>
<p><strong><em>Elbow Criterion (RMSSDT dan RS)</em></strong></p>
<p>Elbow criterion adalah salah satu cara untuk menentukan jumlah cluster yang paling tepat untuk pemodelan k-means. Elbow criterion untuk k-means ini mengkombinasikan antara nilai RMSSTD dan RS statistics, dimana cluster yang paling tepat untuk suatu dataset ditentukan apabila perbedaan nilai antara RMSSTD dan RS menjadi berbanding terbalik dengan keadaan sebelumnya.</p>
<p>RMSSTD (Root Means Square Standard Deviation) merupakan alat ukur tingkat kemiripan (homogeneity) data yang terdapat di dalam cluster yang ditemukan (within clusters). Makin rendah nilai RMSSTD makin mirip data di dalam cluster yang ditemukan. RMSSDT dihitung menggunakan rumus sebagai berikut:</p>
<p>RMSSTD = SQRT (SUM(i=1 to k) SUM(j=1 to d) (SUM(k=1 to N_i) ((x_kj – mu_j)^2)) /
SUM(i=1 to k) SUM(j=1 to d) (N_i – 1))</p>
<p>RS (R Squared) digunakan untuk mengukur tingkat kesamaan atau ketidaksamaan antara cluster (between clusters). RS mempunyai nilai antara 0 dan 1. Nilai 0 untuk cluster yang sama dan 1 untuk cluster yang benar-benar berbeda. RS dihitung dengan rumus:</p>
<p>RS = (SS_t – SS_w) / SS_t</p>
<p>SS_t = SUM(j=1 to d) (SUM(k=1 to N) ((x_kj – mu_j)^2) dan
SS_w = SUM(i=1 to k) SUM(j=1 to d) (SUM(k=1 to N_i) ((x_kj – mu_j)^2))</p>
<p>Notasi:
x_kj : data ke-k yang ada di dalam cluster untuk dimensi ke-j
mu_j : means/rata-rata nilai dari variabel dimensi ke-j
N_i : jumlah data di dalam cluster ke-i
N : jumlah data keseluruhan
d : jumlah dimensi dari data
k : jumlah cluster</p>
<p>Elbow criterion adalah suatu modelling criterion yang bisa digunakan untuk menentukan jumlah cluster dengan melihat perubahan perbandingan antara nilai RMSSTD dan RS. Hal ini dilihat dengan membandingkan persentase tingkat perubahan kedua nilai (RMSSTD dan RS). Apabila muncul suatu keadaan yang berbanding terbalik dengan keadaan sebelumnya, maka titik sebelum terjadinya perubahan tersebut dianggap sebagai jumlah cluster yang paling tepat.</p>
<p>Referensi:
Subhash Sharma: Applied Multivariate Techniques, John Wiley &amp; Sons, Inc., 1996.</p>
<p><strong><em>G-Means (Gaussian Means)</em></strong></p>
<p>G-Means berusaha menentukan jumlah cluster secara gradual dengan melihat apakah suatu cluster sudah dalam keadaan terdistribusi secara normal atau tidak. Kalau masih dianggap belum normal, maka akan dilihat apakah cluster tersebut masih bisa displit untuk dijadikan dua cluster. Hipotesis yang digunakan adalah:</p>
<p>H0: Data di sekitar centroid disample dari suatu distribusi Gaussian.
H1: Data di sekitar centroid tidak disample dari suatu distribusi Gaussian.</p>
<p>Adapun algoritma yang digunakan untuk menentukan split atau tidak adalah sebagai berikut:
1. Tentukan confidence level Alpha untuk testing.
2. Tentukan dua centroid baru c1 dan c2.
3. Jalankan k-means terhadap dua centroid baru tersebut.
4. Tentukan vektor v yang menghubungkan antara centroid c1 dan c2 dimana v = c1 – c2. Kemudian proyeksikan data X ke v dengan rumus x’ = (x,v)/||v||^2. X’ merupakan representasi satu dimensi dari data yang diproyeksikan ke v. Hitung z = F(x’), dimana z adalah bentuk normalisasi x’.
5. Hitung nilai A_2<em>(Z) dengan rumus di bawah. Apabila A_2</em>(Z) berada di wilayah nilai non-critical pada confidence level Alpha, maka H0 diterima, dan centroid awal tetap digunakan dan centroid baru c1 dan c2 dihapus. Untuk keadaan sebaliknya, H0 harus ditolak dan centroid baru c1 dan c2 digunakan sebagai pengganti centroid awal.</p>
<p>A_2<em>(Z) = A_2(Z)(1+4/n-25/(n^2)) dan
A_2(Z) = -1/n * SUM (i=1 to n) ((2</em>i – 1)(log(z_i)+log(1-z_(n+1-i))) – n</p>
<p>dimana:
n : Jumlah data</p>
<p>Referensi:
G. Hamerly and C. Elkan (2003). <em>Learning the k in k-means</em>. In Proceedings of 17th Neural Information Processing Systems.</p>
<p><strong>X-Means</strong>
Merupakan suatu pengembangan k-means untuk menentukan jumlah cluster yang paling tepat untuk suatu dataset yang dianalisa. Adapun algoritma yang diterapkan di dalam metote ini adalah dengan menggabungkan algoritma k-means murni dan salah satu modelling criterion seperti Bayesian Information Criterion (BIC) atau Akaike Information Criterion (AIC) – penjelasan tentang kedua criteria ini ada di <a href="https://yudiagusta.wordpress.com/mixture-modelling/">My Mixture Modelling Page</a>. Algoritma yang diterapkan adalah sebagai berikut:
Step 1: Tentukan jumlah cluster
Step 2: Lakukan optimasi dengan k-means murni
Step 3: Untuk setiap cluster yang dihasilkan split cluster menjadi dua dan bandingkan modelling score antara model dengan satu cluster dan model dengan dua cluster. Model dengan score yang lebih baik dipilih menjadi perwakilan model yang displit.</p>
<p>Referensi
Pelleg D. and Moore A. (2000). X-means: Extending K-means with Efficient Estimation of the Number of Clusters. In Proceedings of the Seventeenth International Conference on Machine Learning (ICML). 727-734.</p>
<p><strong>K-Means Variances</strong></p>
<p><em>Fuzzy c-Means</em>
Variasi dalam hal penentuan nilai keanggotaan data ke masing-masing cluster.</p>
<p>Fuzzy c-Means merupakan perkembangan dari metode k-means dengan memperhitungkan bahwa data dapat tergabung ke dalam ke dalam beberapa cluster dengan tingkat keanggotaan yang berbeda-beda. Tetapi sum total dari nilai keanggotaan data tersebut harus sama dengan 1. Misalnya data x1 bisa tergabung dengan cluster I dengan tingkat keanggotaan 0,7, dengan cluster II dengan tingkat keanggotaan 0,2 dan dengan cluster III dengan tingkat keanggotaan 0,1. Hal ini berbeda dengan k-means karena metode tersebut mengharuskan suatu data untuk menjadi anggota atau tidak sekalian.</p>
<p>Algoritma yang digunakan dalam fuzzy c-means sama dengan algoritma yang digunakan dalam k-means. Tetapi objective function dan rumus yang digunakan untuk menghitung centroid/rata-rata berbeda. Di samping itu, dalam fuzzy c-means, tingkat keanggotaan, yang sering disebut dengan membership function, juga perlu di hitung. Objective function dan rumus-rumus yang digunakan adalah sebagai berikut:</p>
<p>Objective function:</p>
<p>J (U, V) = SUM (k=1 to N) SUM (i=1 to c) (u_ik^m * (x_k, v_i)^2)</p>
<p>dimana:
u_ik = membership function data ke i ke cluster ke k
m = weighting exponent
U, V, N, c, x_k dan v_i : sudah dijelaskan di objective function untuk k-means</p>
<p>Rumus menghitung rata-rata/centroid:</p>
<p>v_ij = SUM (k=1 to N_i) (u_ik^m * x_kj) / SUM (k=1 to N_i) (u_ik^m)</p>
<p>Rumus menghitung membership function:</p>
<p>u_ik = SUM (j=1 to c) ((x_k – v_i)/(x_k – v_j))^(2/(m-1))</p>
<p>Referensi:
Bezdek, J.C. (1981). Pattern Recognition with Fuzzy Objective Function Algorithms. Plenum Press, New York.</p>
<p><em>K Harmonic Means</em>
Variasi dalam hal objective function yang digunakan.</p>
<p>K Harmonic Means menggunakan suatu objective function untuk menghindarkan pengaruh yang besar dari data-data yang ada di sekitar cluster center. Untuk keperluan tersebut, di samping nilai keanggotaan dari data terhadap cluster, setiap data juga mempunyai nilai weight yang digunakan untuk meng-harmoni-kan pengaruh masing-masing data dalam pemodelan. Adapun objective function yang digunakan dalam metode ini adalah sebagai berikut:</p>
<p>J (V) = SUM (k=1 to N) (c / (SUM (i=1 to c) (1/ (x_k- v_i)^p)))</p>
<p>dimana:
V : Matriks centroid/rata-rata masing-masing cluster
N : Jumlah data
c : Jumlah cluster
v_i : Nilai centroid cluster ke-i
p : suatu input parameter yang besarnya lebih dari atau sama dengan 2</p>
<p>Nilai membership function u_ik dan weight w_k dihitung dengan rumus sebagai berikut:</p>
<p>u_ik = SUM (j=1 to c) ((x_k – v_i)/(x_k – v_j))^(-p-2)</p>
<p>w_k = SUM (j=1 to c) (x_k – v_j)^(-p-2) / (SUM (j=1 to c) (x_k – v_j)^(-p))^2</p>
<p>Referensi:
G. Hamerly and C. Elkan (2002). <em>Altenatives to the k-means algorithm that find better clusterings</em>. In Proceedings of the 11th International Conference on Information and Knowledge Management. pp. 600-607.</p>
<p><em>Kernel K-means</em>
Variasi untuk sekumpulan data yang ‘dianggap’ terkelompok secara non-linear.</p>
<p>K-means yang standar umumnya digunakan untuk mengelompokkan data yang secara umum datanya dipisahkan secara linear. Kalau data ‘dianggap’ terkelompok secara non-linear, k-means perlu dimodifikasi untuk dapat mengakomodasi ke-non-linear-an tersebut. Adapun metode yang diimplementasikan di dalam menyelesaikan permasalahan ini adalah dengan mengadopsi Kernel Trick dalam proses pengelompokan yang dilakukan.</p>
<p>Perlu diketahui di sini, untuk bisa menerapkan Kernel Trick, data perlu untuk dipetakan terlebih dahulu ke bentuk data dengan dimensi yang lain, yang umumnya jumlah dimensinya lebih tinggi dengan fungsi THETA(x), yang artinya data x dipetakan dari dimensi awal ke dimensi yang lain (umumnya dimensi yang lebih tinggi).</p>
<p>Adapun objective function yang digunakan untuk Kernel K-Means adalah:</p>
<p>J (U, V) = SUM (k=1 to N) SUM (i=1 to c) (a_ik * (THETA(x_k), v_i)^2)</p>
<p>dimana:
U : Matriks keanggotaan data ke masing-masing cluster yang berisikan nilai 0 dan 1
V : Matriks centroid/rata-rata masing-masing cluster
N : Jumlah data
c : Jumlah cluster
a_ik : Keanggotaan data ke-k ke cluster ke-i
v_i : Nilai centroid cluster ke-i</p>
<p>Rumus menghitung rata-rata/centroid:</p>
<p>v_ij = SUM (k=0 to N_i) THETA (x_kj) / N_i</p>
<p>yang tentunya tidak bisa dihitung, karena kita tidak bisa mengetahui nilai THETA (x_kj) secara langsung. Hal ini bisa diakali, dengan langsung menentukan nilai a_ik (yang nilainya adalah 1 atau 0), dimana a_ik bernilai 1 untuk data yang mempunyai jarak terdekat dengan centroid dan 0 untuk yang lainnya. Adapun jarak antara data dengan centroid dihitung dengan rumus berikut ini:</p>
<p>(THETA(x_l),v_i)^2 = SUM (l=0 to N_i) THETA(x_lj)<em>THETA(x_lj) –
2</em>SUM (k,l=0 to N_i) THETA (x_lj)<em>THETA (x_kj)/ N_i +
SUM (k=0 to N_i) THETA (x_kj)</em>THETA (x_kj)/ N_i</p>
<p>Disini fungsi kernel disubstitusikan, sehingga jarak antara data dengan centroid dapat dihitung dengan rumus berikut ini:</p>
<p>(THETA(x_l),v_i)^2 = SUM (l=0 to N_i) K(x_lj,x_lj) –
2*SUM (k,l=0 to N_i) K (x_lj,x_kj)/ N_i + SUM (k=0 to N_i) K (x_kj,x_kj)/ N_i</p>
<p>Data di-assign ke dalam kelompok yang jarak antara centroid dan data tersebut diminimalkan. Prosedur yang digunakan tetap sama dengan prosedur yang diterapkan pada standar k-means.</p>
<p>Referensi
M Girolami (2002). Mercer kernel-based clustering in feature space. Neural Networks, IEEE Transactions on, Vol. 13, No. 3., pp. 780-784.
Inderjit S Dhillon, Yuqiang Guan, Brian Kulis (2004).Kernel k-means: spectral clustering and normalized cuts. pp. 551-556</p>
<p><em>K-Modes</em>
Variasi dalam hal data yang dianalisa bukan continuous, tapi categorical.</p>
<p>Untuk data categorical, k-means tidak bisa digunakan karena data categorical tidak bisa dicari nilai means (rata-rata)-nya. Untuk keperluan tersebut K-Modes dapat digunakan sebagai gantinya. Adapun algoritma yang digunakan sama dengan k-means dengan beberapa modifikasi sebagai berikut:
Step 1. Tentukan jumlah cluster
Step 2. Alokasikan data ke dalam cluster secara random
Step 3. Hitung modes dari data yang ada di masing-masing cluster.
Step 4. Alokasikan masing-masing data ke cluster terdekat menggunakan <a href="https://yudiagusta.wordpress.com/2008/05/13/similarity-measure/">Hemming Distance</a>
Step 5. Kembali ke Step 3, apabila masih ada data yang berpindah cluster atau apabila perubahan nilai modes atau apabila perubahan nilai pada objective function yang digunakan, di atas nilai threshold yang ditentukan</p>
<p><strong>Objective function</strong> yang digunakan k-modes adalah:</p>
<p>J (A, M) = SUM (k=1 to N) SUM (i=1 to c) (a_ik * D(x_k, m_i))</p>
<p>dimana:
A : Matriks keanggotaan data ke masing-masing cluster yang berisikan nilai 0 dan 1
M : Matriks modes masing-masing cluster
N : Jumlah data
c : Jumlah cluster
a_ik : Keanggotaan data ke-k ke cluster ke-i
x_k : data ke-k
m_i : Nilai modes cluster ke-i
D(x_k, m_i) : <a href="https://yudiagusta.wordpress.com/2008/05/13/similarity-measure/">Hemming Distance </a>antara data x dan modes m</p>
<p>Referensi:
Chaturvedi, A.D., Green, P.E. and Carroll, J.D. (2001). K-Modes Clustering. Journal of Classification, 18, 35-56.</p>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
        
          <a href="getting-started/" title="Implemantasi K-Mean clustering" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Implemantasi K-Mean clustering
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2016 - 2019 Aguskurr
          </div>
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
  <div class="md-footer-social">
    <link rel="stylesheet" href="assets/fonts/font-awesome.css">
    
      <a href="https://github.com/aguskurr" class="md-footer-social__link fa fa-github-alt"></a>
    
      <a href="https://instagram.com/aguskurr_" class="md-footer-social__link fa fa-instagram"></a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="assets/javascripts/application.8c0d971c.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:"."}})</script>
      
    
  </body>
</html>